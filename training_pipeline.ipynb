{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0997d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "114a9360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-usable Components (from previous tasks)\n",
    "\n",
    "class PestCNN(nn.Module):\n",
    "    \"\"\"Implements the 3-layer convolutional neural network (Architecture C)\"\"\"\n",
    "    def __init__(self):\n",
    "        super(PestCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 128)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.output = nn.Linear(128, 17) # 17 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1) # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x) # Logits\n",
    "        return x\n",
    "\n",
    "class JutePestDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading jute pest images on-the-fly.\"\"\"\n",
    "    def __init__(self, df, transform=None, class_to_idx=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['filepath']\n",
    "        label_str = self.df.iloc[idx]['label']\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error loading {img_path}: {e}. Using a dummy image.\")\n",
    "            image = Image.new('RGB', (224, 224))\n",
    "            \n",
    "        label = self.class_to_idx[label_str]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "def load_data_from_folders(dataset_dir):\n",
    "    \"\"\"Scans a directory and loads image paths/labels.\"\"\"\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png']\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    if not dataset_dir.exists():\n",
    "        print(f\"Error: Dataset path not found: {dataset_dir}\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    for class_dir in dataset_dir.iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            class_name = class_dir.name\n",
    "            for img_path in class_dir.rglob('*'):\n",
    "                if img_path.is_file() and img_path.suffix.lower() in image_extensions:\n",
    "                    filepaths.append(str(img_path))\n",
    "                    labels.append(class_name)\n",
    "\n",
    "    if not filepaths:\n",
    "        print(f\"Error: No images found in {dataset_dir}.\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame({'filepath': filepaths, 'label': labels})\n",
    "    df = df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770e6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Script\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Configuration\n",
    "    DATASET_PATH = Path(\"Jute_Pest_Dataset/train\") \n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "    # Design Choices\n",
    "    BATCH_SIZE = 32      \n",
    "    LEARNING_RATE = 1e-3 \n",
    "    NUM_EPOCHS = 3   # Initial test\n",
    "\n",
    "    # Setup device (use GPU if available, else CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Data Loading and Splitting\n",
    "    full_df = load_data_from_folders(DATASET_PATH)\n",
    "    \n",
    "    if full_df.empty:\n",
    "        sys.exit(\"Data loading failed. Exiting.\")\n",
    "\n",
    "    # Create the class-to-integer mapping from the *full* dataset\n",
    "    unique_classes = sorted(full_df['label'].unique())\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(unique_classes)}\n",
    "    \n",
    "    # Split the DataFrame\n",
    "    X = full_df['filepath']\n",
    "    y = full_df['label']\n",
    "    \n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, train_size=0.70, stratify=y, random_state=RANDOM_SEED\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    # Create final DataFrames\n",
    "    train_df = pd.DataFrame({'filepath': X_train, 'label': y_train})\n",
    "    val_df = pd.DataFrame({'filepath': X_val, 'label': y_val})\n",
    "    test_df = pd.DataFrame({'filepath': X_test, 'label': y_test})\n",
    "    \n",
    "    print(f\"Data split: {len(train_df)} train, {len(val_df)} val, {len(test_df)} test\")\n",
    "\n",
    "    # Preprocessing \n",
    "    # Load the pre-calculated stats\n",
    "    try:\n",
    "        TRAIN_MEAN = torch.load(\"train_mean.pt\")\n",
    "        TRAIN_STD = torch.load(\"train_std.pt\")\n",
    "        print(\"Loaded pre-calculated normalization statistics.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: Statistics files not found. Using ImageNet defaults.\")\n",
    "        # Fallback to ImageNet stats if not calculated\n",
    "        TRAIN_MEAN = torch.tensor([0.485, 0.456, 0.406])\n",
    "        TRAIN_STD = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "    data_transforms = {\n",
    "        'train': T.Compose([\n",
    "            T.RandomResizedCrop(224),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(TRAIN_MEAN, TRAIN_STD)\n",
    "        ]),\n",
    "        'val': T.Compose([\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(TRAIN_MEAN, TRAIN_STD)\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    # Component 1: DataLoaders\n",
    "    train_dataset = JutePestDataset(\n",
    "        train_df, transform=data_transforms['train'], class_to_idx=class_to_idx\n",
    "    )\n",
    "    val_dataset = JutePestDataset(\n",
    "        val_df, transform=data_transforms['val'], class_to_idx=class_to_idx\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Component 2: Model, Loss, Optimizer\n",
    "    model = PestCNN().to(device)\n",
    "    \n",
    "    # Loss function (combines LogSoftmax and NLLLoss)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "\n",
    "    # Component 3 & 4: Training & Validation Loop \n",
    "    \n",
    "    # Lists to store metrics\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training Phase \n",
    "        model.train() # Set model to training mode\n",
    "        running_train_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            # Move data to the correct device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss (multiply by batch size for correct averaging)\n",
    "            running_train_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_loss_history.append(epoch_train_loss)\n",
    "\n",
    "        # Validation Phase \n",
    "        model.eval() # Set model to evaluation mode\n",
    "        running_val_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        \n",
    "        with torch.no_grad(): # Disable gradient calculation\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_preds += torch.sum(preds == labels.data)\n",
    "                \n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = correct_preds.double() / len(val_loader.dataset)\n",
    "        \n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "        val_acc_history.append(epoch_val_acc.item()) # Store as float\n",
    "        \n",
    "        # Print Epoch Results \n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "              f\"Time: {(epoch_end_time - epoch_start_time):.2f}s | \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f} | \"\n",
    "              f\"Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "    print(\"\\nFinished 3-epoch test run.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0997d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114a9360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-usable Components (from previous tasks)\n",
    "\n",
    "class PestCNN(nn.Module):\n",
    "    \"\"\"Implements the 3-layer convolutional neural network (Architecture C)\"\"\"\n",
    "    def __init__(self):\n",
    "        super(PestCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 128)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.output = nn.Linear(128, 17) # 17 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1) # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x) # Logits\n",
    "        return x\n",
    "\n",
    "class JutePestDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading jute pest images on-the-fly.\"\"\"\n",
    "    def __init__(self, df, transform=None, class_to_idx=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['filepath']\n",
    "        label_str = self.df.iloc[idx]['label']\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error loading {img_path}: {e}. Using a dummy image.\")\n",
    "            image = Image.new('RGB', (224, 224))\n",
    "            \n",
    "        label = self.class_to_idx[label_str]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "def load_data_from_folders(dataset_dir):\n",
    "    \"\"\"Scans a directory and loads image paths/labels.\"\"\"\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png']\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    if not dataset_dir.exists():\n",
    "        print(f\"Error: Dataset path not found: {dataset_dir}\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    for class_dir in dataset_dir.iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            class_name = class_dir.name\n",
    "            for img_path in class_dir.rglob('*'):\n",
    "                if img_path.is_file() and img_path.suffix.lower() in image_extensions:\n",
    "                    filepaths.append(str(img_path))\n",
    "                    labels.append(class_name)\n",
    "\n",
    "    if not filepaths:\n",
    "        print(f\"Error: No images found in {dataset_dir}.\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame({'filepath': filepaths, 'label': labels})\n",
    "    df = df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770e6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Script\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Configuration\n",
    "    DATASET_PATH = Path(\"Jute_Pest_Dataset/train\") \n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "    # Design Choices\n",
    "    BATCH_SIZE = 32      \n",
    "    LEARNING_RATE = 1e-3 \n",
    "    NUM_EPOCHS = 20  \n",
    "\n",
    "    # Setup device (use GPU if available, else CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Data Loading and Splitting\n",
    "    full_df = load_data_from_folders(DATASET_PATH)\n",
    "    \n",
    "    if full_df.empty:\n",
    "        sys.exit(\"Data loading failed. Exiting.\")\n",
    "\n",
    "    # Create the class-to-integer mapping from the *full* dataset\n",
    "    unique_classes = sorted(full_df['label'].unique())\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(unique_classes)}\n",
    "    \n",
    "    # Split the DataFrame\n",
    "    X = full_df['filepath']\n",
    "    y = full_df['label']\n",
    "    \n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, train_size=0.70, stratify=y, random_state=RANDOM_SEED\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    # Create final DataFrames\n",
    "    train_df = pd.DataFrame({'filepath': X_train, 'label': y_train})\n",
    "    val_df = pd.DataFrame({'filepath': X_val, 'label': y_val})\n",
    "    test_df = pd.DataFrame({'filepath': X_test, 'label': y_test})\n",
    "    \n",
    "    print(f\"Data split: {len(train_df)} train, {len(val_df)} val, {len(test_df)} test\")\n",
    "\n",
    "    # Preprocessing \n",
    "    # Load the pre-calculated stats\n",
    "    try:\n",
    "        TRAIN_MEAN = torch.load(\"train_mean.pt\")\n",
    "        TRAIN_STD = torch.load(\"train_std.pt\")\n",
    "        print(\"Loaded pre-calculated normalization statistics.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: Statistics files not found. Using ImageNet defaults.\")\n",
    "        # Fallback to ImageNet stats if not calculated\n",
    "        TRAIN_MEAN = torch.tensor([0.485, 0.456, 0.406])\n",
    "        TRAIN_STD = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "    data_transforms = {\n",
    "        'train': T.Compose([\n",
    "            T.RandomResizedCrop(224),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(TRAIN_MEAN, TRAIN_STD)\n",
    "        ]),\n",
    "        'val': T.Compose([\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(TRAIN_MEAN, TRAIN_STD)\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    # Component 1: DataLoaders\n",
    "    train_dataset = JutePestDataset(\n",
    "        train_df, transform=data_transforms['train'], class_to_idx=class_to_idx\n",
    "    )\n",
    "    val_dataset = JutePestDataset(\n",
    "        val_df, transform=data_transforms['val'], class_to_idx=class_to_idx\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Component 2: Model, Loss, Optimizer\n",
    "    model = PestCNN().to(device)\n",
    "    \n",
    "    # Loss function (combines LogSoftmax and NLLLoss)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "\n",
    "    # Component 3 & 4: Training & Validation Loop \n",
    "    \n",
    "    # Lists to store metrics\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training Phase \n",
    "        model.train() # Set model to training mode\n",
    "        running_train_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            # Move data to the correct device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss (multiply by batch size for correct averaging)\n",
    "            running_train_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_loss_history.append(epoch_train_loss)\n",
    "\n",
    "        # Validation Phase \n",
    "        model.eval() # Set model to evaluation mode\n",
    "        running_val_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        \n",
    "        with torch.no_grad(): # Disable gradient calculation\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_preds += torch.sum(preds == labels.data)\n",
    "                \n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = correct_preds.double() / len(val_loader.dataset)\n",
    "        \n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "        val_acc_history.append(epoch_val_acc.item()) # Store as float\n",
    "        \n",
    "        # Print Epoch Results \n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "              f\"Time: {(epoch_end_time - epoch_start_time):.2f}s | \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f} | \"\n",
    "              f\"Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "    print(\"\\nFinished 20-epoch test run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeea059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Configuration for Comparison \n",
    "optimizer_configs = {\n",
    "    'SGD_lr0.01_mom0.0': {'optimizer': 'sgd', 'lr': 0.01, 'momentum': 0.0},\n",
    "    'SGD_lr0.01_mom0.5': {'optimizer': 'sgd', 'lr': 0.01, 'momentum': 0.5},\n",
    "    'SGD_lr0.01_mom0.9': {'optimizer': 'sgd', 'lr': 0.01, 'momentum': 0.9},\n",
    "    'SGD_lr0.01_mom0.99': {'optimizer': 'sgd', 'lr': 0.01, 'momentum': 0.99},\n",
    "}\n",
    "\n",
    "# Result Storage \n",
    "# Manually add our first Adam run (assuming lists are in memory)\n",
    "results = {\n",
    "    'Adam_lr0.001': {\n",
    "        'train_loss': train_loss_history, \n",
    "        'val_loss': val_loss_history,     \n",
    "        'val_acc': val_acc_history,       \n",
    "        'final_val_acc': val_acc_history[-1],\n",
    "        'training_time': 22.16 * 20 \n",
    "    }\n",
    "}\n",
    "\n",
    "NUM_EPOCHS = 20 \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# Main Experiment Loop \n",
    "for name, config in optimizer_configs.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training with {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Create a fresh, re-initialized model for a fair comparison\n",
    "    model = PestCNN().to(device)\n",
    "    \n",
    "    # Create optimizer based on config\n",
    "    optimizer = optim.SGD(model.parameters(), \n",
    "                            lr=config['lr'], \n",
    "                            momentum=config['momentum'])\n",
    "    \n",
    "    # Lists to store metrics for *this specific run*\n",
    "    run_train_loss = []\n",
    "    run_val_loss = []\n",
    "    run_val_acc = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_train_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        run_train_loss.append(epoch_train_loss)\n",
    "\n",
    "        #  Validation Phase\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_preds += torch.sum(preds == labels.data)\n",
    "                \n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = correct_preds.double() / len(val_loader.dataset)\n",
    "        \n",
    "        run_val_loss.append(epoch_val_loss)\n",
    "        run_val_acc.append(epoch_val_acc.item()) # Store as float\n",
    "        \n",
    "        # Print Epoch Results \n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "              f\"Time: {(epoch_end_time - epoch_start_time):.2f}s | \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f} | \"\n",
    "              f\"Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # Store this run's results\n",
    "    results[name] = {\n",
    "        'train_loss': run_train_loss,\n",
    "        'val_loss': run_val_loss,\n",
    "        'val_acc': run_val_acc,\n",
    "        'final_val_acc': run_val_acc[-1],\n",
    "        'training_time': total_time\n",
    "    }\n",
    "\n",
    "print(\"\\nOptimizer comparison complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c34059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_optimizer_comparison(results, metric='val_loss', title='Validation Loss Comparison'):\n",
    "    \"\"\"\n",
    "    Plot a specific metric for all optimizers on the same graph.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 7))\n",
    "        \n",
    "    for name, data in results.items():\n",
    "        epochs = range(1, len(data[metric]) + 1)\n",
    "        plt.plot(epochs, data[metric], marker='o', label=name, linewidth=2, markersize=4)\n",
    "        \n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel(metric.replace('_', ' ').title(), fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "print(\"Generating comparison plots...\")\n",
    "\n",
    "# Plot 1: Validation Accuracy\n",
    "plot_optimizer_comparison(results, metric='val_acc',\n",
    "                          title='Validation Accuracy: Optimizer Comparison')\n",
    "\n",
    "# Plot 2: Validation Loss\n",
    "plot_optimizer_comparison(results, metric='val_loss',\n",
    "                          title='Validation Loss: Optimizer Comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f15e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_epoch_at_threshold(acc_history, threshold=0.50):\n",
    "    \"\"\"Finds the first epoch (1-indexed) to cross a threshold.\"\"\"\n",
    "    for i, acc in enumerate(acc_history):\n",
    "        if acc >= threshold:\n",
    "            return i + 1\n",
    "    return \"N/A\" # Return N/A if threshold is never met\n",
    "\n",
    "def get_loss_stability(loss_history, last_n_epochs=5):\n",
    "    \"\"\"Calculates std dev of loss over the last N epochs.\"\"\"\n",
    "    if len(loss_history) < last_n_epochs:\n",
    "        return np.std(loss_history)\n",
    "    return np.std(loss_history[-last_n_epochs:])\n",
    "\n",
    "def create_results_table(results):\n",
    "    \"\"\"Create a summary table of optimizer performance.\"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    for name, data in results.items():\n",
    "        summary_data.append({\n",
    "            'Optimizer': name,\n",
    "            'Final Val Acc (%)': f\"{data['val_acc'][-1] * 100:.2f}\",\n",
    "            'Best Val Acc (%)': f\"{max(data['val_acc']) * 100:.2f}\",\n",
    "            'Epoch @ 50% Acc': get_epoch_at_threshold(data['val_acc']),\n",
    "            'Loss Stability (Std)': f\"{get_loss_stability(data['val_loss']):.4f}\",\n",
    "            'Total Time (s)': f\"{data['training_time']:.1f}\",\n",
    "            'Time per Epoch (s)': f\"{data['training_time'] / NUM_EPOCHS:.2f}\"\n",
    "        })\n",
    "        \n",
    "    df = pd.DataFrame(summary_data)\n",
    "    df = df.sort_values('Best Val Acc (%)', ascending=False)\n",
    "    return df\n",
    "\n",
    "# Create and display the table\n",
    "print(\"Generating results summary table...\")\n",
    "results_table = create_results_table(results)\n",
    "print(results_table.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
